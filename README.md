# Web-Crawler-Project
The goal of this project to explore a real website, collect statistics, and understand how crawlers handle URL discovery, page parsing, and word frequency analysis.


**What It Does
**
* Visits and records all unique URL pages

* Identifies the longest page by word count

* Finds the 50 most frequent words across all pages

* Tracks how many pages exist under each subdomain

Example of what it would look like if you run it for specific set of URLs:

**Results**

Unique pages visited: 10175

Longest page:

URL: http://www.example.com/~cs224

Word count: 104,961

**Top 50 Most Frequent Words**

events (27623), search (13679), isg (10738), april (9989), day (9925),
research (9696), talks (8015), reunion (8004), outlook (7991), next (7562),
data (7362), us (6338), news (6290), read (5884), received (5786),
upcoming (5639), navigation (5602), talk (5527), views (5495), file (5484),
calendar (5437), enter (5321), export (5286), information (5264),
systems (5019), may (4989), jump (4810), scheduled (4714),
contact (3894), database (3853), students (3741), faculty (3648),
award (3538), projects (3516), group (3497), people (3487),
best (3272), two (3219), courses (3182), home (3150), ieee (3092),
welcome (3087), like (3024), publications (3007), fellow (2947),
recent (2941)


**Subdomains**
accessibility.example.com (5)
archive.example.com (183)
asterix.example.com (5)
cecs.example.com (10)
cert.example.com (8)
....


